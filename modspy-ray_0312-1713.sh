#!/bin/bash
# shellcheck disable=SC2206
# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!
# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE RUNNABLE!

#SBATCH --account=def-mtarailo
#SBATCH --job-name=modspy-ray_0312-1713
#SBATCH -e ./logs/%J-modspy-ray_0312-1713.err
#SBATCH -o ./logs/%J-modspy-ray_0312-1713.out

#SBATCH --time=00-23:59:00           # time (DD-HH:MM:SS)
#SBATCH --mem=80GB                   # memory; default unit is megabytes

### This script works for any number of nodes, Ray will find and manage all resources
#SBATCH --nodes=4
####SBATCH --exclusive
### Give all resources to a single Ray task, ray can manage the resources internally
#SBATCH --tasks-per-node=1
#SBATCH --gpus-per-task=1
#SBATCH --cpus-per-task=4
#SBATCH --mail-user=kmtahsinhassan.rahit@ucalgary.ca
#SBATCH --mail-type=ALL


# Load modules or your own conda environment here
# module load pytorch/v1.4.0-gpu
# conda activate ${CONDA_ENV}
module --force purge
module load StdEnv/2020
module load gcc/9.3.0 python/3.11.5 cuda/11.8.0 arrow/12.0.1 scipy-stack/2023b
source ./py311

export CUDA_LAUNCH_BLOCKING=1
export MAIN_NODE=$(hostname)
export NCCL_BLOCKING_WAIT=1 #Pytorch Lightning uses the NCCL backend for inter-GPU communication by default. Set this variable to avoid timeout errors.

## Create a virtualenv and install Ray on all nodes ##
# srun -N $SLURM_NNODES -n $SLURM_NNODES config_env.sh

# ===== DO NOT CHANGE THINGS HERE UNLESS YOU KNOW WHAT YOU ARE DOING =====
# This script is a modification to the implementation suggest by gregSchwartz18 here:
# https://github.com/ray-project/ray/issues/826#issuecomment-522116599
redis_password=$(uuidgen)
export redis_password

nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST") # Getting the node names
nodes_array=($nodes)

node_1=${nodes_array[0]}
ip=$(srun --nodes=1 --ntasks=1 -w "$node_1" hostname --ip-address) # making redis-address

# if we detect a space character in the head node IP, we'll
# convert it to an ipv4 address. This step is optional.
if [[ "$ip" == *" "* ]]; then
  IFS=' ' read -ra ADDR <<< "$ip"
  if [[ ${#ADDR[0]} -gt 16 ]]; then
    ip=${ADDR[1]}
  else
    ip=${ADDR[0]}
  fi
  echo "IPV6 address detected. We split the IPV4 address as $ip"
fi

export HEAD_NODE=$(hostname) # store head node's address
port=6379
ip_head=$ip:$port
export ip_head
echo "IP Head: $ip_head"


# # Start prometheus and grafana
# export RAY_GRAFANA_HOST="http://${HEAD_NODE}:3000"
# export RAY_PROMETHEUS_HOST="http://${HEAD_NODE}:9090"
# # export RAY_PROMETHEUS_NAME="Prometheus"
# # export RAY_GRAFANA_IFRAME_HOST="http://${hostname}:3000"

# # Start prometheus
# cd /home/rahit/projects/def-mtarailo/rahit/prometheus-2.51.1.linux-amd64
# ./prometheus --config.file=/home/rahit/projects/def-mtarailo/rahit/from_scratch/modspy-data/conf/prometheus.yml &
# sleep 10
# # Start grafana
# cd /home/rahit/projects/def-mtarailo/rahit/grafana-v10.4.1
# ./bin/grafana-server --config /home/rahit/projects/def-mtarailo/rahit/from_scratch/modspy-data/conf/grafana.ini web &
# sleep 10
# cd /home/rahit/projects/def-mtarailo/rahit/from_scratch/modspy-data

# Start the head
echo "STARTING HEAD at $node_1"
srun --nodes=1 --ntasks=1 -w "$node_1" \
  ray start --head --node-ip-address="$ip" --port=$port --redis-password="$redis_password" --dashboard-host="$ip" --dashboard-port=8265 --temp-dir="/scratch/rahit/ray" --block &
sleep 30
echo "$node_1: Ray head started!"

worker_num=$((SLURM_JOB_NUM_NODES - 1)) #number of nodes other than the head node
for ((i = 1; i <= worker_num; i++)); do
  node_i=${nodes_array[$i]}
  echo "STARTING WORKER $i at $node_i"
  srun --nodes=1 --ntasks=1 -w "$node_i" ray start --address "$ip_head" --redis-password="$redis_password" --block &
  sleep 5
  echo "$i at $node_i: Ray worker started!"
done


# ===== Call your code below =====
python ./src/modspy_data/models/modspy.py --n-samples=16
# python ./src/modspy_data/models/modspy-monarch.py --n-samples=1